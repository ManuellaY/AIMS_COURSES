{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpIRG11RUDs7"
   },
   "source": [
    "## **Intro to Pytorch**\n",
    "\n",
    "PyTorch is an open-source machine learning library developed by Facebook's AI Research lab (FAIR). It is widely used for various machine learning tasks, including deep learning. PyTorch is known for its dynamic computational graph, which allows for more intuitive and flexible model development compared to static computational graphs used in some other frameworks.\n",
    "\n",
    "Follow instructions in the link:\n",
    "https://pytorch.org/tutorials/beginner/introyt/introyt1_tutorial.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAT_-DDoYM_S"
   },
   "source": [
    "**Tensors**: Similar to TensorFlow, PyTorch uses tensors as the fundamental building blocks for numerical computations. Tensors are multi-dimensional arrays that can be used for various mathematical operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6htZbbY0YSTD"
   },
   "source": [
    "**Autograd**: PyTorch includes an automatic differentiation library called Autograd. This feature automatically computes gradients of tensors with respect to a given objective, which is essential for training machine learning models through techniques like backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1VEhfvJYWwN"
   },
   "source": [
    "**Neural Network Module**: PyTorch provides a torch.nn module that simplifies the process of building and training neural networks. It includes pre-defined layers, loss functions, and optimization algorithms, making it easier for developers to construct and train models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JA4-pjPwUVP8"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8db725a56e03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqcUrVE4kvVq"
   },
   "source": [
    "## **Manipulating Tensors in PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h96M0dkBk5DS",
    "outputId": "67efdc05-e061-499c-cf95-75ab847aa665"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [3, 4]\n",
    "x_data = torch.tensor(data)\n",
    "x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHio3NEilQHH"
   },
   "source": [
    "Tensors can be created from NumPy arrays and vice versa. This allows for seamless integration with existing NumPy code and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-mSLOAe7lSwW"
   },
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mhotDvlln62"
   },
   "source": [
    "Using built-in functions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uUGv5HOCZPov"
   },
   "outputs": [],
   "source": [
    "# Tensor with random values between 0 and 1\n",
    "rand_tensor = torch.rand(2, 3)\n",
    "\n",
    "# Tensor with random values from a standard normal distribution\n",
    "randn_tensor = torch.randn(2, 3)\n",
    "\n",
    "# Tensor with all ones\n",
    "ones_tensor = torch.ones(2, 3)\n",
    "\n",
    "# Tensor with all zeros\n",
    "zeros_tensor = torch.zeros(2, 3)\n",
    "\n",
    "# Identity matrix\n",
    "eye_tensor = torch.eye(3)\n",
    "\n",
    "# Tensor with values from 0 to 9 (exclusive)\n",
    "arange_tensor = torch.arange(10)\n",
    "\n",
    "# Tensor with 10 values evenly spaced between 0 and 1\n",
    "linspace_tensor = torch.linspace(0, 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwM9TKJ0l5pm"
   },
   "source": [
    "## **Tensor Attributes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WkWEVBZl-dg"
   },
   "source": [
    "Understanding tensor attributes is crucial for effective manipulation:\n",
    "\n",
    "\n",
    "\n",
    "*   Shape: A tuple of tensor dimensions, accessed using tensor. shape or` tensor.size()`. This determines the number of elements and their arrangement within the tensor.\n",
    "*   Data type: The type of data stored in the tensor (e.g., torch.float32, torch.int64), accessed using `tensor.dtype`. This affects the precision and memory usage of the tensor.\n",
    "\n",
    "*   Device: The device where the tensor is stored (CPU or GPU), accessed using `tensor.device`. This is important for utilizing GPU acceleration for faster computations.\n",
    "*   Layout: The way the tensor is stored in memory (e.g., strided, sparse), accessed using `tensor.layout`. This can affect performance and memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNlzqP0zmsyf"
   },
   "source": [
    "## **Tensor Operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vy6ZbU10m0Yc"
   },
   "source": [
    "PyTorch supports a wide range of tensor operations:\n",
    "\n",
    "Indexing and slicing: Accessing specific elements or sub-tensors using standard indexing and slicing techniques. This allows for extracting and manipulating parts of the tensor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f37uwK9IcLU8",
    "outputId": "f054111e-e1f9-4f7b-c4aa-f490046ce9fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row: tensor([1., 1., 1., 1.])\n",
      "First column: tensor([1., 1., 1., 1.])\n",
      "Last column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "print(f\"First row: {tensor[0]}\")\n",
    "print(f\"First column: {tensor[:, 0]}\")\n",
    "print(f\"Last column: {tensor[..., -1]}\")\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5VZNRbXnLYo"
   },
   "source": [
    "Reshaping: Changing the shape of a tensor without altering its data using methods like `tensor.view()`, `tensor.reshape()`, and `tensor.transpose()`. This is often used to adapt tensors for different operations or layers in a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r7piYEVCnH9Y",
    "outputId": "e6a2af38-f5eb-4375-d122-4de74d7d8163"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zP__UIeMnhZC"
   },
   "source": [
    "Concatenation: Joining tensors along a given dimension using `torch.cat()` or `torch.stack()`. This is useful for combining data or features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dDz9R4donVTW",
    "outputId": "c93edd8b-0474-46bb-f6fb-b853c7489383"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "# Create two tensors\n",
    "tensor1 = torch.tensor([1, 2, 3])\n",
    "tensor2 = torch.tensor([4, 5, 6])\n",
    "\n",
    "# Concatenate along the existing axis (dim=0)\n",
    "concatenated_tensor = torch.cat((tensor1, tensor2), dim=0)\n",
    "stacked_tensor = torch.stack((tensor1, tensor2), dim=0)\n",
    "print(concatenated_tensor)\n",
    "print(stacked_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqO5V83eoHZA"
   },
   "source": [
    "Mathematical operations: Performing arithmetic, linear algebra, matrix manipulation, and other mathematical operations on tensors. PyTorch provides a comprehensive set of functions for these operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "paxK738Rn1Di",
    "outputId": "7371a3cb-1fbd-4bed-be42-80967562e0c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6,  7,  8],\n",
       "        [ 9, 10, 11]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix multiplication\n",
    "y1 = stacked_tensor @ stacked_tensor.T\n",
    "y2 = stacked_tensor.matmul(stacked_tensor.T)\n",
    "\n",
    "# Element-wise multiplication\n",
    "z1 = stacked_tensor * stacked_tensor\n",
    "z2 = stacked_tensor.mul(stacked_tensor)\n",
    "\n",
    "# Element-wise addition\n",
    "stacked_tensor.add_(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGF8DfwCo9mb"
   },
   "source": [
    "In-place operations: Operations that modify the tensor directly, denoted by an underscore postfix (e.g., `tensor.add_()`). These can be more memory-efficient but should be used with caution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0RDCMoJpGUO"
   },
   "source": [
    "## **Using the Autograd Module**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWUKG5EfpMuA"
   },
   "source": [
    "The autograd module is PyTorch's automatic differentiation engine, crucial for training neural networks. This section covers gradients, Jacobians, Hessians, and computational graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qa0t8soxpQ4o"
   },
   "source": [
    "## **1. Gradients**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jV2PJzCXpXWU"
   },
   "source": [
    "requires_grad: To compute gradients, set the `requires_grad=True` for a tensor. This enables tracking of operations on that tensor, creating a computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "5A36H0pdoKGq"
   },
   "outputs": [],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "keOaiBKvphwM"
   },
   "source": [
    "`.backward() `: Computes the gradients of a tensor with respect to its leaf nodes in the computational graph. This is the core of backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mx91Gu1Ope6R",
    "outputId": "ef293868-d885-4e48-c101-a7333a85a589"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NV0D0xJSppDt"
   },
   "source": [
    "`.grad`: Accesses the accumulated gradients for a tensor. This attribute stores the gradients computed during the `.backward()` call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5UxgCAIpzBk"
   },
   "source": [
    "## **2. Jacobian Product**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRAN6LwLp6Dr"
   },
   "source": [
    "For vector-valued functions, autograd computes the Jacobian product, which is the product of the Jacobian matrix and a given vector. This is more efficient than computing the full Jacobian matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lqPDMqlopllI",
    "outputId": "e8975c06-2ced-4f7b-90f2-25dab6a717a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.,  0.],\n",
      "        [ 0., 12.]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define a function\n",
    "def func(x):\n",
    "    return torch.stack([x[0]**2, x[1]**3])\n",
    "\n",
    "# Create an input tensor with requires_grad=True\n",
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "\n",
    "# Compute the output of the function\n",
    "y = func(x)\n",
    "\n",
    "# Compute the Jacobian\n",
    "jacobian = torch.zeros((2, 2))  # initialize a tensor to hold the Jacobian matrix\n",
    "for i in range(2):\n",
    "    y[i].backward(retain_graph=True)\n",
    "    jacobian[i] = x.grad\n",
    "    x.grad.zero_()  # zero the gradients for the next computation\n",
    "\n",
    "print(jacobian)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S37s1jsHqUWL"
   },
   "source": [
    "## **3. Hessians**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kmA0cAFqYaO"
   },
   "source": [
    "The Hessian matrix represents the second derivatives of a function, you can use the `torch.autograd.functional.hessian()` function to calculate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a_az1gekqGLJ",
    "outputId": "81e2a4ae-0624-4a87-a749-179ad1e45842"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 0.],\n",
      "        [0., 2.]])\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd.functional import hessian\n",
    "\n",
    "def f(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "hessian_matrix = hessian(f, x)\n",
    "print(hessian_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QV7xCbEjqsc1"
   },
   "source": [
    "## **4. Computational Graph**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCMxiL0pqxrK"
   },
   "source": [
    "Autograd maintains a directed acyclic graph (DAG) representing the operations performed on tensors. This graph is dynamic and recreated after each `.backward()` call. Understanding this graph is crucial for debugging and optimizing your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bq_eW8fFq2QO"
   },
   "source": [
    "## **5.  Disabling Gradient Tracking**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxs8N0TmrAry"
   },
   "source": [
    "Gradient tracking can be disabled using `torch.no_grad()` to speed up computations when gradients are not needed, such as during inference or when evaluating a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qbU6lfugqorB",
    "outputId": "8b77ad5d-545b-4077-b5f4-ef737ffa5f8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
